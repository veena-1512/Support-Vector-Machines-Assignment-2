{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e664188-3c6c-4315-b6b5-f8f0668b7dc7",
   "metadata": {},
   "source": [
    "Q1.What is the relationship between polynomial functions and kernel functions in machine learning algorithms? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b604c5-3373-44e2-a5b0-a91b50dc94e7",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both mathematical tools used in machine learning, particularly in the context of kernel methods, which are a class of algorithms that operate on feature spaces. Here's a brief overview of their relationship:\n",
    "\n",
    "Polynomial Functions:\n",
    "\n",
    "A polynomial function is a mathematical function that can be expressed as the sum of powers of a variable, typically in the form of f(x) = a_n * x^n + a_(n-1) * x^(n-1) + ... + a_1 * x + a_0, where \"x\" is the variable, \"a\" represents coefficients, and \"n\" is a non-negative integer.\n",
    "In machine learning, polynomial functions can be used to create features by raising the original input features to various powers (e.g., x^2, x^3) and combining them. This is often done to capture non-linear relationships between features.\n",
    "\n",
    "Kernel Functions:\n",
    "\n",
    "Kernel functions, in the context of machine learning, are used in kernel methods such as Support Vector Machines (SVMs) and kernelized versions of algorithms like Principal Component Analysis (PCA) and Ridge Regression.\n",
    "Kernel functions are used to implicitly compute the dot product (inner product) between feature vectors in a higher-dimensional space, without explicitly transforming the data into that space. This allows algorithms to work in a feature space where the data may be linearly separable or easier to work with.\n",
    "\n",
    "\n",
    "Relationship between Polynomial Functions and Kernel Functions:\n",
    "\n",
    "i. Polynomial kernels are a specific type of kernel function used in kernel methods. They are designed to capture polynomial relationships between data points without explicitly calculating the polynomial expansion of features.\n",
    "\n",
    "ii. The polynomial kernel function typically has the form K(x, y) = (x ⋅ y + c)^d, where \"x\" and \"y\" are feature vectors, \"c\" is a constant, and \"d\" is the degree of the polynomial.\n",
    "\n",
    "iii. When you use a polynomial kernel in a machine learning algorithm like an SVM, it effectively computes the dot product in a higher-dimensional feature space, and this dot product is equivalent to a polynomial function of the original data.\n",
    "\n",
    "iv. This allows kernelized algorithms to model complex non-linear relationships between data points using polynomial functions, even when the data itself is not explicitly transformed into a higher-dimensional space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1925a-e1e5-446d-93ca-dcc5c453f661",
   "metadata": {},
   "source": [
    "Q2.How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c604e5f-9189-4e35-9ef7-7d146ef77f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59476ec0-f387-48e0-be65-a8d953253c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d683640-78f8-4737-accb-8e585a356022",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.  How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e77b55-49d0-4bae-be5c-70714086ed77",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), the parameter epsilon (often denoted as ε) is a critical hyperparameter that determines the width of the ε-insensitive tube around the predicted values. Support vectors are data points that lie either on the boundary of this tube or outside of it. The choice of epsilon can influence the number of support vectors in an SVR model. Here's how increasing the value of epsilon affects the number of \n",
    "support vectors:\n",
    "\n",
    "1. Small Epsilon (Tight Tube): When you set a small value for epsilon, the ε-insensitive tube becomes narrow. In this case, SVR is more sensitive to fitting the data closely. This can lead to a larger number of support vectors, as the model tries to capture fine details in the training data.\n",
    "\n",
    "2. Large Epsilon (Wide Tube): Conversely, when you increase the value of epsilon, the ε-insensitive tube becomes wider. A wider tube allows for more errors or deviations between the predicted values and the actual target values within the tube. This can result in fewer support vectors, as the model is more tolerant of errors and focuses on capturing the overall trend in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fb9e119-d832-4b18-8092-52ea58d40b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon=0.1: Number of Support Vectors = 8\n",
      "Epsilon=0.5: Number of Support Vectors = 2\n",
      "Epsilon=1.0: Number of Support Vectors = 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# Create SVR models with different epsilon values\n",
    "epsilon_values = [0.1, 0.5, 1.0]\n",
    "support_vector_counts = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    svr = SVR(kernel='rbf', epsilon=epsilon)\n",
    "    svr.fit(X, y)\n",
    "    support_vector_counts.append(len(svr.support_))\n",
    "\n",
    "for epsilon, count in zip(epsilon_values, support_vector_counts):\n",
    "    print(f\"Epsilon={epsilon}: Number of Support Vectors = {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97593231-7988-49cc-9c52-ea8fd2c21518",
   "metadata": {},
   "source": [
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter \n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works \n",
    "and provide examples of when you might want to increase or decrease its value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65de56-b4f4-4cb1-ad14-f20b22b56eed",
   "metadata": {},
   "source": [
    "upport Vector Regression (SVR) is a powerful machine learning technique for regression tasks. The choice of kernel function, along with the C, epsilon (ε), and gamma (γ) parameters, can significantly affect the performance of an SVR model. Let's discuss each parameter and how they impact the model, along with examples of when you might want to increase or decrease their values:\n",
    "\n",
    "1. Kernel Function (Kernel):\n",
    "\n",
    "The kernel function determines the type of transformation applied to the input features. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "\n",
    "Choice of Kernel:\n",
    "i. Linear Kernel ('linear'): Use when the relationship between input features and the target variable is roughly linear.\n",
    "\n",
    "ii. Polynomial Kernel ('poly'): Use when the relationship is polynomial and you specify the degree of the polynomial using the degree parameter.\n",
    "\n",
    "iii. RBF Kernel ('rbf'): Suitable for capturing non-linear and complex relationships in the data, often a good default choice.\n",
    "\n",
    "iv. Sigmoid Kernel ('sigmoid'): Use when you believe the relationship is sigmoidal in nature.\n",
    "\n",
    "Example: If you suspect that your data exhibits a complex, non-linear pattern, you might choose the RBF kernel.\n",
    "\n",
    "2. C Parameter (C):\n",
    "\n",
    "i. The C parameter controls the trade-off between maximizing the margin and minimizing the classification error on the training data.\n",
    "\n",
    "ii. Smaller C values result in a larger margin but may allow more training points to violate the ε-insensitive tube.\n",
    "\n",
    "iii. Larger C values make the model stricter, reducing the number of support vectors but potentially leading to overfitting.\n",
    "\n",
    "Example: If you have noisy data and want to prevent overfitting, you might decrease C. If you have a clean dataset and want a strict fit, you might increase C.\n",
    "\n",
    "3. Epsilon Parameter (epsilon):\n",
    "\n",
    "i. Epsilon defines the width of the ε-insensitive tube around the predicted values. It controls the tolerance for errors in the training data.\n",
    "\n",
    "ii. A smaller epsilon makes the tube narrower, and the model is more sensitive to fitting the training data closely.\n",
    "\n",
    "iii. A larger epsilon makes the tube wider, allowing more training points to be within the tube.\n",
    "Example: If you have data with noise or outliers and want the model to be more robust, you might increase epsilon.\n",
    "\n",
    "\n",
    "4. Gamma Parameter (gamma):\n",
    "\n",
    "i. The gamma parameter influences the shape of the RBF kernel and the flexibility of the model.\n",
    "\n",
    "ii. Smaller gamma values result in a more flexible and smooth decision boundary, which can lead to underfitting.\n",
    "\n",
    "iii. Larger gamma values make the boundary more rigid, which can lead to overfitting.\n",
    "\n",
    "Example: If you want to avoid overfitting in a high-dimensional dataset, you might decrease gamma. If you want to capture fine details in the data, you might increase gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5df15-eb5e-41ae-9a27-1dc2b46228dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c7977a-a5ca-4fe7-9dc0-d0fc7e6a02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef09094d-2808-405d-b5cd-5dc988014a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "   target  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "iris_df = pd.DataFrame(data=np.c_[iris['data'], iris['target']],\n",
    "                       columns=iris['feature_names'] + ['target'])\n",
    "\n",
    "# Print the first few rows of the dataset (optional)\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "278275c4-eeeb-4abd-9b3e-d20a6ac0e355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (X_train, y_train): (105, 4) (105,)\n",
      "Testing set shape (X_test, y_test): (45, 4) (45,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target \n",
    "\n",
    "# Split the dataset into training and testing sets (e.g., 70% training, 30% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Print the shapes of the training and testing sets to verify the split\n",
    "print(\"Training set shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Testing set shape (X_test, y_test):\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04972c44-e58f-49b6-8fe8-c3fe53b0aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the Min-Max Scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "print(X_normalized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f1f8c62-cf23-4641-bb7c-eaa162770fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC(kernel='linear', C=1.0)  # You can specify the kernel and C value\n",
    "\n",
    "# Train the SVC classifier on the training data\n",
    "svc_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68b68624-eb22-4bf6-95fb-a27279cbc2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create and train an SVM classifier\n",
    "svm_classifier = SVC(kernel='linear') \n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4387a65-011d-480c-8be2-06f6057ce112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-Score: 1.00\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-Score: {f1:.2f}\")\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df451726-3239-4734-9162-757d4e4c4db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'degree': 2, 'gamma': 0.001, 'kernel': 'linear'}\n",
      "Accuracy with Best Model: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Define a parameter grid to search through\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],              \n",
    "    'kernel': ['linear', 'rbf'],   \n",
    "    'gamma': [0.001, 0.01, 0.1],  \n",
    "    'degree': [2, 3, 4]            \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Get the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the best model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy with Best Model: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9239c86-24d6-42b9-94e8-a0c4b318f68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 1, 'gamma': 0.01, 'kernel': 'linear'}\n",
      "Accuracy on the entire dataset: 0.9933333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets (you can skip this step if you want to use the entire dataset for training)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],            \n",
    "    'kernel': ['linear', 'rbf'], \n",
    "    'gamma': [0.01, 0.1, 1],   \n",
    "}\n",
    "svm_classifier = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation to find the best hyperparameters\n",
    "grid_search = GridSearchCV(svm_classifier, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svm_classifier = SVC(**best_params)\n",
    "best_svm_classifier.fit(X, y)\n",
    "\n",
    "# Make predictions on the entire dataset (you can use a separate test set)\n",
    "y_pred = best_svm_classifier.predict(X)\n",
    "\n",
    "# Evaluate the model's performance on the entire dataset\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy on the entire dataset:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37f66d-a17b-49bd-bf53-0eb6602523d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01319dc3-a267-4e9f-8ebd-72512db561be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149bba4-7061-4a32-bf35-9f75cb82d1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071c208-7c98-4daf-aff4-68cc53490c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
